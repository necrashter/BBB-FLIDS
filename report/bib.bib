@misc{FLforIDSsurvey,
Author = {Shaashwat Agrawal and Sagnik Sarkar and Ons Aouedi and Gokul Yenduri and Kandaraj Piamrat and Sweta Bhattacharya and Praveen Kumar Reddy Maddikunta and Thippa Reddy Gadekallu},
Title = {Federated Learning for Intrusion Detection System: Concepts, Challenges and Future Directions},
Year = {2021},
Eprint = {arXiv:2106.09527},
}

@article{IDSusingMLsurvey,
author = {KishorWagh, Sharmila and Pachghare, Vinod and Kolhe, Satish},
year = {2013},
month = {09},
pages = {30-37},
title = {Survey on Intrusion Detection System using Machine Learning Techniques},
volume = {78},
journal = {International Journal of Computer Applications},
doi = {10.5120/13608-1412}
}

@misc{bcflsurvey,
Author = {Zhilin Wang and Qin Hu},
Title = {Blockchain-based Federated Learning: A Comprehensive Survey},
Year = {2021},
Eprint = {arXiv:2110.02182},
}


@misc{solidity,
  author = {Ethereum},
  title = {{Solidity v0.8.13 Documentation}},
  howpublished = "\url{https://docs.soliditylang.org/en/v0.8.13/}",
  year = {2022}, 
  note = "[Online; accessed 2022-05-11]"
}

@misc{solidityalt,
  author = {Ethereum},
  title = {{Smart Contract Languages}},
  howpublished = "\url{https://ethereum.org/en/developers/docs/smart-contracts/languages/}",
  year = {2022}, 
  note = "[Online; accessed 2022-05-11]"
}

@article{flsurvey,
title = {A survey on federated learning},
journal = {Knowledge-Based Systems},
volume = {216},
pages = {106775},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106775},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121000381},
author = {Chen Zhang and Yu Xie and Hang Bai and Bin Yu and Weihong Li and Yuan Gao},
keywords = {Federated learning, Privacy protection, Machine learning},
abstract = {Federated learning is a set-up in which multiple clients collaborate to solve machine learning problems, which is under the coordination of a central aggregator. This setting also allows the training data decentralized to ensure the data privacy of each device. Federated learning adheres to two major ideas: local computing and model transmission, which reduces some systematic privacy risks and costs brought by traditional centralized machine learning methods. The original data of the client is stored locally and cannot be exchanged or migrated. With the application of federated learning, each device uses local data for local training, then uploads the model to the server for aggregation, and finally the server sends the model update to the participants to achieve the learning goal. To provide a comprehensive survey and facilitate the potential research of this area, we systematically introduce the existing works of federated learning from five aspects: data partitioning, privacy mechanism, machine learning model, communication architecture and systems heterogeneity. Then, we sort out the current challenges and future research directions of federated learning. Finally, we summarize the characteristics of existing federated learning, and analyze the current practical application of federated learning.}
}


@inproceedings{comef-FL,
title = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
author  = {H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Aguera y Arcas},
year  = {2017},
URL = {http://arxiv.org/abs/1602.05629},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)}
}


@misc{favoriteDataset,
  author = {Sampada Bhosale},
  title = {{Network Intrusion Detection Dataset}},
  howpublished = "\url{https://www.kaggle.com/datasets/sampadab17/network-intrusion-detection}",
  year = {2018}, 
  note = "[Online; accessed 2022-07-06]"
}

@article{standardizationANN,
  title={The Influence of Input Data Standardization Method on Prediction Accuracy of Artificial Neural Networks},
  author={Hubert Anysz and A. Zbiciak and Nabi Ibadov},
  journal={Procedia Engineering},
  year={2016},
  volume={153},
  pages={66-70}
}

@inproceedings{Tikhomirov2017EthereumSO,
  title={Ethereum: State of Knowledge and Research Perspectives},
  author={Sergei Tikhomirov},
  booktitle={FPS},
  year={2017}
}

@article{ColabIDS,
title = {Enhancing collaborative intrusion detection via disagreement-based semi-supervised learning in IoT environments},
journal = {Journal of Network and Computer Applications},
volume = {161},
pages = {102631},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102631},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520301053},
author = {Wenjuan Li and Weizhi Meng and Man Ho Au},
keywords = {Collaborative intrusion detection, Semi-supervised learning, False alarm reduction, Detection performance, Internet of things},
abstract = {Collaborative intrusion detection systems (CIDSs) are developing to improve the detection performance of a single detector in Internet of Things (IoT) networks, through exchanging and sharing data. For anomaly detection, machine learning is an important and essential tool to help identify the deviation between current events and pre-built profile. For a traditional supervised learning classifier, there is a need to provide training examples with ground-truth labels in advance. However, labeled instances are quite limited in real-world IoT scenarios, while unlabeled data/instances are widely available. This is because data labeling is a very expensive process that requires huge human efforts and knowledge inputs. To mitigate this issue, the use of semi-supervised learning algorithms is a promising solution, which can leverage unlabeled data to label data automatically without human intervention. In this work, we focus on semi-supervised learning and design DAS-CIDS, by applying disagreement-based semi-supervised learning algorithm for CIDSs. In the evaluation, we investigate the performance of DAS-CIDS using both datasets and in real IoT network environments, in the aspects of both detection performance and false alarm reduction. The experimental results show that as compared with traditional supervised classifiers, our approach is more effective in detecting intrusions and reducing false alarms by automatically leveraging unlabeled data.}
}

@inproceedings{mt-dnn-fl,
author = {Zhao, Ying and Chen, Junjun and Wu, Di and Teng, Jian and Yu, Shui},
title = {Multi-Task Network Anomaly Detection Using Federated Learning},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369705},
doi = {10.1145/3368926.3369705},
abstract = {Because of the complexity of network traffic, there are various significant challenges in the network anomaly detection fields. One of the major challenges is the lack of labeled training data. In this paper, we use federated learning to tackle data scarcity problem and to preserve data privacy, where multiple participants collaboratively train a global model. Unlike the centralized training architecture, participants do not need to share their training to the server in federated learning, which can prevent the training data from being exploited by attackers. Moreover, most of the previous works focus on one specific task of anomaly detection, which restricts the application areas and can not provide more valuable information to network administrators. Therefore, we propose a multi-task deep neural network in federated learning (MT-DNN-FL) to perform network anomaly detection task, VPN (Tor) traffic recognition task, and traffic classification task, simultaneously. Compared with multiple single-task models, the multi-task method can reduce training time overhead. Experiments conducted on well-known CICIDS2017, ISCXVPN2016, and ISCXTor2016 datasets, show that the detection and classification performance achieved by the proposed method is better than the baseline methods in centralized training architecture.},
booktitle = {Proceedings of the Tenth International Symposium on Information and Communication Technology},
pages = {273â€“279},
numpages = {7},
keywords = {Deep Neural Networks, Federated Learning, Network Anomaly Detection},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT 2019}
}

@INPROCEEDINGS{FLwirelessIDS,  author={Cetin, Burak and Lazar, Alina and Kim, Jinoh and Sim, Alex and Wu, Kesheng},  booktitle={2019 IEEE International Conference on Big Data (Big Data)},   title={Federated Wireless Network Intrusion Detection},   year={2019},  volume={},  number={},  pages={6004-6006},  doi={10.1109/BigData47090.2019.9005507}}

@inproceedings{FLperf,
author = {Nilsson, Adrian and Smith, Simon and Ulm, Gregor and Gustavsson, Emil and Jirstrand, Mats},
title = {A Performance Evaluation of Federated Learning Algorithms},
year = {2018},
isbn = {9781450361194},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286490.3286559},
doi = {10.1145/3286490.3286559},
abstract = {Federated learning is an approach to distributed machine learning where a global model is learned by aggregating models that have been trained locally on data-generating clients. Contrary to centralized optimization, clients can be very large in number and face challenges of data and network heterogeneity. Examples of clients include smartphones and connected vehicles, which highlights the practical relevance of federated learning. We benchmark three federated learning algorithms and compare their performance against a centralized approach where data resides on the server. The algorithms Federated Averaging (FedAvg), Federated Stochastic Variance Reduced Gradient, and CO-OP are evaluated on the MNIST dataset, using both i.i.d. and non-i.i.d. partitionings of the data. Our results show that FedAvg achieves the highest accuracy among the federated algorithms, regardless of how data was partitioned. Our comparison between FedAvg and centralized learning shows that they are practically equivalent when i.i.d. data is used. However, the centralized approach outperforms FedAvg with non-i.i.d. data.},
booktitle = {Proceedings of the Second Workshop on Distributed Infrastructures for Deep Learning},
pages = {1â€“8},
numpages = {8},
keywords = {Algorithm evaluation, Federated learning, Machine learning},
location = {Rennes, France},
series = {DIDL '18}
}

  
@INPROCEEDINGS{FL-HC,  author={Briggs, Christopher and Fan, Zhong and Andras, Peter},  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},   title={Federated learning with hierarchical clustering of local updates to improve training on non-IID data},   year={2020},  volume={},  number={},  pages={1-9},  doi={10.1109/IJCNN48605.2020.9207469}}

@misc{kdd,
  author = {{University of California, Irvine}},
  title = {{KDD Cup 1999 Data}},
  howpublished = "\url{https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html}",
  year = {1999}, 
  note = "[Online; accessed 2022-07-08]"
}

@ARTICLE{dbf,  author={Alkadi, Osama and Moustafa, Nour and Turnbull, Benjamin and Choo, Kim-Kwang Raymond},  journal={IEEE Internet of Things Journal},   title={A Deep Blockchain Framework-Enabled Collaborative Intrusion Detection for Protecting IoT and Cloud Networks},   year={2021},  volume={8},  number={12},  pages={9463-9472},  doi={10.1109/JIOT.2020.2996590}}